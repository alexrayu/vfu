{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Web scraping performance - Requests + BeautifulSoup VS Scrapy\n",
    "\n",
    "## Our task here.\n",
    "\n",
    "Subjective observations followed by some early measurements have indicated, that\n",
    "Scrapy is around 4-5 times faster than Requests + BS solution (R+BS). Being\n",
    "built on top of an async framework, scrapy is processing multiple requests\n",
    "asynchronously, greatly increasing the process bandwidth.\n",
    "See https://docs.scrapy.org/en/latest/intro/overview.html#what-just-happened\n",
    "\n",
    "Here we will measure how Scrapy's performance varies in the case of 100 requests,\n",
    "and how this compares with R+BS performance in the same test.\n",
    "\n",
    "## Subject 1: Scrapy.\n",
    "\n",
    "Scrapy's default setting is CONCURRENT_REQUESTS_PER_DOMAIN = 8."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-11 09:24:15 [scrapy.utils.log] INFO: Scrapy 2.5.0 started (bot: scrapybot)\n",
      "2021-08-11 09:24:15 [scrapy.utils.log] INFO: Versions: lxml 4.6.3.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.7.0, Python 3.8.10 (default, Jun  2 2021, 10:49:15) - [GCC 9.4.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.11.0-25-generic-x86_64-with-glibc2.29\n",
      "2021-08-11 09:24:15 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2021-08-11 09:24:15 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'LOG_ENABLED': False,\n",
      " 'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scrapy 100 items in 7s\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "import datetime\n",
    "\n",
    "# 1 item.\n",
    "class BikeSpider(scrapy.Spider):\n",
    "    name = 'bikespider'\n",
    "    start_urls = [\n",
    "        'https://bazar.bg/obiavi/gradski-velosipedi/varna?condition=2',\n",
    "    ] * 100\n",
    "    custom_settings = {\n",
    "       'LOG_ENABLED': False,\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        for ad in response.css('.awrapper .listItemContainer .listItemLink'):\n",
    "            pass\n",
    "\n",
    "\n",
    "# A single process will be reused.\n",
    "process = CrawlerProcess({\n",
    "    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n",
    "})\n",
    "\n",
    "# Run 1 item.\n",
    "begin_time = datetime.datetime.now()\n",
    "process.crawl(BikeSpider)\n",
    "process.start()\n",
    "diff = datetime.datetime.now() - begin_time\n",
    "print(f'Scrapy 100 items in {diff.seconds}s')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Subject 1: R+BS.\n",
    "\n",
    "R+BS has no concurrency. It processes 1 item at a time. 100 items will take\n",
    "about 2 minutes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R+BS 100 items in 103s\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "begin_time = datetime.datetime.now()\n",
    "\n",
    "base_url = 'https://bazar.bg/obiavi/gradski-velosipedi/varna?condition=2'\n",
    "count_pages = 100\n",
    "data = []\n",
    "for i in range(count_pages):\n",
    "    cur_page = i + 1\n",
    "    print(f'\\rPage {cur_page} of {count_pages}', end='\\r')\n",
    "    response = requests.get(base_url)\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    css_selector = '.awrapper .listItemContainer .listItemLink'\n",
    "    for item in soup.select(css_selector):\n",
    "        pass\n",
    "\n",
    "diff = datetime.datetime.now() - begin_time\n",
    "print(f'R+BS 100 items in {diff.seconds}s          ')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}