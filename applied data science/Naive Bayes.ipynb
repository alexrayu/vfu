{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "**Naive Bayes (NB)** is a family of probability classifiers based on the Probability Theory by Thomas Bayes. It's calles **naive** or **independent** becase it's based on an assumptions that the estimated **features** (also called vectors, or problem instances) are not causally connected.\n",
    "\n",
    "*Example:* If an object has features of being red, round, 10cm in diameter, these features independently add to the probability of it calssified as an apple.\n",
    "\n",
    "**Naive Bayes** is a *supervised* ML argorithm, it needs to be trained with labeled data before it can work.\n",
    "**Naive Bayes** is based on the formula of *Conditional Probability*.\n",
    "\n",
    "**Kernel Density Estimation** is a function with smoothes probability density of a variable. It is widely used with NB, and together they make NB very competitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "NB relies on causal (posterior) probablility \"A because B\" \n",
    "\n",
    "### Formula for conditional probability\n",
    "\n",
    "$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$\n",
    "\n",
    "- $P(A \\mid B)$ - Conditional probability. Probability of $A$ occuring given $B$.\n",
    "- $P(B \\mid A)$ - Probbility of $B$ ocurring if $A$ is true - *detected causal probability* - this is the probabilities glimpsed from analisys.\n",
    "- $P(A)$ and $P(B)$ are probabilities of observing $A$ and $B$ without any conditions (marginal probability).\n",
    "\n",
    "Thus, humanly speaking, posterior probability is observed causal probability multiplied by probability of observed effect divided by probability of observed cause:\n",
    "\n",
    "$P(A \\mid B) = \\frac{P(B \\cap A)}{P(B)}$\n",
    "\n",
    "The conditional probability is the joined probability (union) of $A$ and $B$ occuring, divided by the marginal probability of $B$ occuring. Where $B$ is cause, $A$ is effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and cons\n",
    "\n",
    "Naive Bayes has some pros and cons, which influence it's use.\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Easy** to implement, easy to maintain.\n",
    "- **Performant** (due to feature independence). Does not require expensive equipment.\n",
    "- Can work with **categorical inputs**, not only numberic.\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **Zero-frequency problem**: can not handle categories which are not in dataset.\n",
    "- **Not very precise** in it's results.\n",
    "- Taking each feature independently, it **misses the connections** between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "Naive Bayes is used for classification in cases where text is involved, performance is required, and the features interdependence recognition id not critical. With text classification, NB is known to be reliable and performant, and this is the main area of its application.\n",
    "\n",
    "- **Sentiment analysis**. What sentiment does the text reflect?\n",
    "- **SPAM analysis**. Is the text spam or ham?\n",
    "- **Recommendations systems**. Used with *collaborative filtering* to predict if a user is likely to use a product based on the list of their currently used products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['chapter', '0'], ['vince', '0'], ['things', '0'], ['well', '0'], ['hope', '0'], ['latest', '0'], ['version', '0'], ['part', '0'], ['chapter', '0'], ['think', '0']]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from functools import reduce\n",
    "import sklearn.metrics\n",
    "\n",
    "token_re = re.compile(r\"\\$?\\d*(?:[.,]\\d+)+|\\w+-\\w+|\\w+\", re.U)\n",
    "\n",
    "# Getting stopwords for each use is *very* slow, so prepare them here.\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "def _tokenize(text):\n",
    "    return list(filter(lambda s: len(s) > 2 and s not in stopwords, token_re.findall(text.lower())))\n",
    "\n",
    "# Import and tokenize the dataset.\n",
    "with open('assets/emails.csv') as fh:\n",
    "    data = list(csv.reader(fh))\n",
    "\n",
    "# Split the data.\n",
    "x_train, x_test = train_test_split(data[1:], test_size=.2, shuffle=True)\n",
    "\n",
    "# Tokenize the train dataset.\n",
    "for i, entry in enumerate(x_train):\n",
    "    x_train[i][0] = _tokenize(entry[0])\n",
    "\n",
    "# Flatten the dataset.\n",
    "flat_data = []\n",
    "for entry in x_train:\n",
    "    for token in entry[0]:\n",
    "        flat_data.append([token, entry[1]])\n",
    "\n",
    "# Flat dataset contains tokens with spam indicator (1 or 0).\n",
    "# There can be multiple cases on of token in this flat list.\n",
    "print(flat_data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chapter', [2, 160]), ('vince', [1, 6867]), ('things', [58, 226]), ('well', [127, 758]), ('hope', [25, 544])]\n"
     ]
    }
   ],
   "source": [
    "# 0 - spam count, 1 - ham count, 2 - spam index, 3 - ham index\n",
    "model = {}\n",
    "spam_total = 0\n",
    "ham_total = 0\n",
    "\n",
    "# Count spam/ham per token.\n",
    "for entry in flat_data:\n",
    "    if not entry[0] in model:\n",
    "        model[entry[0]] = [0, 0]\n",
    "    if entry[1] == '1':\n",
    "        model[entry[0]][0] += 1\n",
    "        spam_total += 1\n",
    "    else:\n",
    "        model[entry[0]][1] += 1\n",
    "        ham_total += 1\n",
    "\n",
    "# Now, model has a token, it's spam and ham counts.\n",
    "print(list(model.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chapter', [2, 160, 0.018292682926829267]), ('vince', [1, 6867, 0.0002911208151382824]), ('things', [58, 226, 0.2062937062937063]), ('well', [127, 758, 0.14430665163472378]), ('hope', [25, 544, 0.04553415061295972])]\n"
     ]
    }
   ],
   "source": [
    "# Smooth spam probability index per word.\n",
    "for token in model:\n",
    "    model[token].append((model[token][0] + 1) / (model[token][0] + model[token][1] + 2))\n",
    "     \n",
    "# Probability of spam and ham.\n",
    "prob_spam = spam_total / (spam_total + ham_total)\n",
    "prob_ham = ham_total / (spam_total + ham_total)\n",
    "\n",
    "# Each token now has a \"spam\" index value between 1 and 0.\n",
    "print(list(model.items())[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"really\": 0.06775089249265955\n",
      "\"identity\": 0.8140876430074708\n",
      "\"abracadabra\": 0.4999970085028959\n"
     ]
    }
   ],
   "source": [
    "# Classification function.\n",
    "def calculate_word(word):\n",
    "    test_si = model[word][2] if word in model else 1 / (spam_total + 2)\n",
    "    test_hi = (1 - model[word][2]) if word in model else 1 / (ham_total + 2)\n",
    "    return (test_si * prob_spam) / ((test_si * prob_spam) + (test_hi * prob_ham))\n",
    "\n",
    "# Test some words.\n",
    "words = ['really', 'identity', 'abracadabra']\n",
    "for word in words:\n",
    "    print('\"' + word + '\": ' + str(calculate_word(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"really\": 0.06775089249265953\n",
      "\"identity\": 0.8140876430074708\n",
      "\"abracadabra\": 0.49999700850289597\n"
     ]
    }
   ],
   "source": [
    "# Calculate spam probability of a text.\n",
    "def calculate_text(text):\n",
    "    words = _tokenize(text)\n",
    "    ratings = []\n",
    "    for word in words:\n",
    "        ratings.append(calculate_word(word))\n",
    "        \n",
    "    # Google B https://patents.google.com/patent/US7523168\n",
    "    spamminess = (pow(reduce(lambda x, y: (1 - x) * (1 - y), ratings), 1 / len(ratings))).real\n",
    "    hamminess = (1 - pow(reduce(lambda x, y: x * y, ratings), 1 / len(ratings))).real\n",
    "    combined = (spamminess - hamminess) / (spamminess + hamminess)\n",
    "    normalized = (1 + combined) / 2\n",
    "   \n",
    "    return normalized\n",
    "\n",
    "\n",
    "# Classify text. A wrapper around calculate_text.\n",
    "def classify(text):\n",
    "    spam_prob = calculate_text(text)\n",
    "    return True if spam_prob > 0.51 else False  \n",
    "\n",
    "words = ['really', 'identity', 'abracadabra']\n",
    "for word in words:\n",
    "    print('\"' + word + '\": ' + str(calculate_text(word)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[258, 29], [15, 844]]\n",
      "p = 0.9616055846422339\n"
     ]
    }
   ],
   "source": [
    "# Validate with confusion matrix.\n",
    "confusion_matrix = [[0, 0], [0, 0]]\n",
    "\n",
    "for entry in x_test[:10]:\n",
    "    response = calculate_text(entry[0])\n",
    "    # print(str(response) + ':' + str(classify(entry[0])) + ':' + str(entry[1]))\n",
    "\n",
    "for entry in x_test:\n",
    "    response = classify(entry[0])\n",
    "    \n",
    "    # True positive.\n",
    "    if response and entry[1] == '1':\n",
    "        confusion_matrix[0][0] += 1\n",
    "        \n",
    "    # False positive.\n",
    "    elif response and entry[1] == '0':\n",
    "        confusion_matrix[0][1] += 1\n",
    "        \n",
    "    # True negative.\n",
    "    elif not response and entry[1] == '0':\n",
    "        confusion_matrix[1][1] += 1\n",
    "    \n",
    "    # False negative.\n",
    "    elif not response and entry[1] == '1':\n",
    "        confusion_matrix[1][0] += 1\n",
    "\n",
    "print(confusion_matrix)\n",
    "print('p = ' + str((confusion_matrix[0][0] + confusion_matrix[1][1]) / len(x_test)))\n",
    "\n",
    "# Publish in \"TEXT MINING\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
