{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "**Naive Bayes (NB)** is a family of probability classifiers based on the Probability Theory by Thomas Bayes. It's calles **naive** or **independent** becase it's based on an assumptions that the estimated **features** (also called vectors, or problem instances) are not causally connected.\n",
    "\n",
    "*Example:* If an object has features of being red, round, 10cm in diameter, these features independently add to the probability of it calssified as an apple.\n",
    "\n",
    "**Naive Bayes** is a *supervised* ML argorithm, it needs to be trained with labeled data before it can work.\n",
    "**Naive Bayes** is based on the formula of *Conditional Probability*.\n",
    "\n",
    "**Kernel Density Estimation** is a function with smoothes probability density of a variable. It is widely used with NB, and together they make NB very competitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "\n",
    "NB relies on causal (posterior) probablility \"A because B\" \n",
    "\n",
    "### Formula for conditional probability\n",
    "\n",
    "$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{P(B)}$\n",
    "\n",
    "- $P(A \\mid B)$ - Conditional probability. Probability of $A$ occuring given $B$.\n",
    "- $P(B \\mid A)$ - Probbility of $B$ ocurring if $A$ is true - *detected causal probability* - this is the probabilities glimpsed from analisys.\n",
    "- $P(A)$ and $P(B)$ are probabilities of observing $A$ and $B$ without any conditions (marginal probability).\n",
    "\n",
    "Thus, humanly speaking, posterior probability is observed causal probability multiplied by probability of observed effect divided by probability of observed cause:\n",
    "\n",
    "$P(A \\mid B) = \\frac{P(B \\cap A)}{P(B)}$\n",
    "\n",
    "The conditional probability is the joined probability (union) of $A$ and $B$ occuring, divided by the marginal probability of $B$ occuring. Where $B$ is cause, $A$ is effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and cons\n",
    "\n",
    "Naive Bayes has some pros and cons, which influence it's use.\n",
    "\n",
    "### Pros\n",
    "\n",
    "- **Easy** to implement, easy to maintain.\n",
    "- **Performant** (due to feature independence). Does not require expensive equipment.\n",
    "- Can work with **categorical inputs**, not only numberic.\n",
    "\n",
    "### Cons\n",
    "\n",
    "- **Zero-frequency problem**: can not handle categories which are not in dataset.\n",
    "- **Not very precise** in it's results.\n",
    "- Taking each feature independently, it **misses the connections** between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application\n",
    "\n",
    "Naive Bayes is used for classification in cases where text is involved, performance is required, and the features interdependence recognition id not critical. With text classification, NB is known to be reliable and performant, and this is the main area of its application.\n",
    "\n",
    "- **Sentiment analysis**. What sentiment does the text reflect?\n",
    "- **SPAM analysis**. Is the text spam or ham?\n",
    "- **Recommendations systems**. Used with *collaborative filtering* to predict if a user is likely to use a product based on the list of their currently used products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"really\": 0.06778882250337476\n",
      "\"identity\": 0.8483490098283131\n",
      "\"abracadabra\": 0.4999976030742031\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "token_re = re.compile(r\"\\$?\\d*(?:[.,]\\d+)+|\\w+-\\w+|\\w+\", re.U)\n",
    "\n",
    "# Getting stopwords for each use is *very* slow, so prepare them here.\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "\n",
    "def _tokenize(text):\n",
    "    return list(filter(lambda s: len(s) > 2 and s not in stopwords, token_re.findall(text.lower())))\n",
    "\n",
    "# Import and tokenize the dataset.\n",
    "with open('assets/emails.csv') as fh:\n",
    "    data = list(csv.reader(fh))[1:]\n",
    "for i, entry in enumerate(data):\n",
    "    data[i][0] = _tokenize(entry[0])\n",
    "\n",
    "# Flatten the dataset.\n",
    "flat_data = []\n",
    "for entry in data:\n",
    "    for token in entry[0]:\n",
    "        flat_data.append([token, entry[1]])\n",
    "\n",
    "# 0 - spam count, 1 - ham count, 2 - spam index, 3 - ham index\n",
    "model = {}\n",
    "spam_total = 0\n",
    "ham_total = 0\n",
    "\n",
    "# Count spam/ham per token.\n",
    "for entry in flat_data:\n",
    "    if not entry[0] in model:\n",
    "        model[entry[0]] = [0, 0]\n",
    "    if entry[1] == '1':\n",
    "        model[entry[0]][0] += 1\n",
    "        spam_total += 1\n",
    "    else:\n",
    "        model[entry[0]][1] += 1\n",
    "        ham_total += 1\n",
    "\n",
    "# Smooth spam probability index per word.\n",
    "for token in model:\n",
    "    model[token].append((model[token][0] + 1) / (model[token][0] + model[token][1] + 2))\n",
    "     \n",
    "# Probability of spam and ham.\n",
    "prob_spam = spam_total / (spam_total + ham_total)\n",
    "prob_ham = ham_total / (spam_total + ham_total)\n",
    "\n",
    "# Classification.\n",
    "words = ['really', 'identity', 'abracadabra']\n",
    "\n",
    "for word in words:\n",
    "    \n",
    "    # Test word spam and ham index.\n",
    "    test_si = model[word][2] if word in model else 1 / (spam_total + 2)\n",
    "    test_hi = (1 - model[word][2]) if word in model else 1 / (ham_total + 2)\n",
    "\n",
    "    # Bayes formula.\n",
    "    spam_prob = (test_si * prob_spam) / ((test_si * prob_spam) + (test_hi * prob_ham))\n",
    "\n",
    "    print('\"' + word + '\": ' + str(spam_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
